# MY_AL_Learning
记录研二第一学期的学习过程

---
## 新想法：
### 1. 数据增强能否对图片局部进行处理
   比如原来是一张图片整体旋转25°/35°...，但是有的增强是无用的，能否只对图片中的有用信息做旋转，然后模仿MixMatch的方法，
   多次增强之后，用分类器预测，再平均+sharpen。
### 2. Mix主动学习
   把不同query方法的得分，平均再sharpen。
### 3. 强化学习+少样本+自监督
### 4. 强化学习+遗传算法
解决收敛速度慢的问题；

---
## 研究生计划

| 时间              | 事情                                                         |
| ----------------- | ------------------------------------------------------------ |
| 研二上（-1.10）   | 1.写专利，修够学术分；2.跑毕设实验；                         |
| 寒假（1.10-2.20） |                                                              |
| 研二下（3月-7月） | 1.准备实习（刷Leetcode）；2.准备国考（网课+习题）            |
| 暑假（8月）       | 实习                                                         |
| 研三上（9月-1月） | 1.准备11月国考；2.11月写完+改毕业论文（12月交）；3.开学找提前批工作； |
| 寒假（2月）       |                                                              |
| 研三下（3月-7月） | 1.毕业答辩；2.省考；                                         |

## 2021/12计划

1.IJCAI会议结稿日期Sat Jan 15 2022 19:59:59 GMT+0800（2022/1/15）

计划12月15日前实验做的差不多，12月16日开始写论文。

2.12月16号去拿居住证。

3.组会大约在1月7号讲；

### 周工作安排（12/6-12/10）

1. 缩短训练周期（12/6）；
2. 调整policy embedding（12/7-12/8）；
3. 12月10号去拿暂住证。
4. 12月24号之前给唐英鹏师兄弄好专利修改；

#### 2021/12/10

【余5天】

- 今日工作：

  1. 重跑BC；

  2. 和师兄讨论policy embedding实验结果；

     后续工作：

     （1）在Ant/Hopper以及更多环境下跑policy embedding实验，包括cat论文中的实验；

     Hopper/Ant&cat/policy embedding;

     估计22：00跑完cat/policy embedding;

     （2）初步修改inverse module的目标函数；

     原来是MSE，现在加上一项$-I(S,Z)$​。

     需要假设Z为高斯分布，训练inverse module类似VAE一样；

     （3）最终希望优化的目标函数为：

     $I(A,Z)-I(A,S|Z)-I(S,Z)-MSE(A,A^*)$​；

     可以尝试优化互信息下界；

  3. 和老师讨论policy embedding实验结果；

     老师提议尝试类似2019_ICCV_Domain Intersection and Domain Difference这篇文章的方法，利用VAE解耦policy_embedding和appearance_embedding（表观信息可以理解为共有信息）；

     如果能实现Decoder(st_ae, st+1_pe)=st+1，说明我能够分解st信息；

     再用(st_pe, at)作为discriminator的输入；

- 明日安排：

  1. 尝试初步修改inverse module的目标函数；

     原来是MSE，现在加上一项$-I(S,Z)$​。

     需要假设Z为高斯分布，训练inverse module类似VAE一样；

#### 2021/12/9

【余6天】

- 今日工作：

  1. 发现policy embedding最后收敛了，True_rewards不再增加;

  ![image-20211209093057613](/home/hehua/.config/Typora/typora-user-images/image-20211209093057613.png)

  2. 发现每轮迭代都要保存一下模型，现在改为save_per_iter: 1000，每1000次迭代/每小时保存一次模型。【OK】

     现在平均迭代100次/min，比save_per_iter: 1节约了至少2/3的时间（33次/min）。

     因此当num_iters=15000时，大概需要150min=3h。

  3. 重新跑一下policy embedding和cat，设置num_iters=15000(差不多100个epoch, 估计14:00跑完)

     观察cat的True reward是否有收敛现象，测试最后一个模型性能。

     如果存在收敛现象，那么可以减少迭代次数到收敛，如num_iters=3000(差不多2个epoch)

     cat&num_iters=15000，共花了2.5h左右，但是没有收敛，因此重新跑cat&num_iters=30000，大概19:30跑完。

  4. policy_embedding&num_iters=15000，BC出来最好只有1432.7093701186，重跑policy_embedding&num_iters=30000；

     估计21：30跑完；

  5. num_iters=15000来看，cat-policy-embedding的方法比cat好。

  6. BC：

     cat-policy-embedding&num_iters=30000(305994)->Failed

     cat&num_iters=30000(307005)->Failed

     不能nohup跑两个设置，会出错；只能单个跑，

     BC验证速度5min/model，因此，num_iters=30000时，保存模型30个，大概需要2.5h。

- 明日安排：

  1. 和师兄讨论policy embedding实验结果；
  2. 尝试在noise network和BDM上加policy embedding，看实验结果；
  3. 看论文；

#### 2021/12/7

【余8天】

- 今日工作：

  1. 昨天晚上跑了10h的policy emdedding，才到4000步/15000步感觉还是很慢啊！

     但是有个好现象就是可以通过True-reward，来观察结果是否收敛。

     发现，修改后的policy embedding迭代1.5k大概需要1h，迭代3k需要5h，迭代4.5k需要11h.

     怎么迭代越久，迭代速度越慢？不应该是同样的迭代速度吗？

     再看cat原来训练图，基本保持在14h/1k步的速度，并不会很明显的后期迭代速度变慢。

     发现保存模型要很长时间？

  2. 合并augmentation+BC为augBC;

     遇到graph和session不匹配问题；

     通过改prefix解决了问题，实现增广一次，BC训练一次，BC验证一次。

  3. 分析policy_embedding和cat对比结果

     发现，并非aug_avgret高的avgret就高，说明并不是所有的增广都是有益的，增广也分好坏；

- 明日安排：

#### 2021/12/6

【余9天】

- 今日工作：

  1. 跑policy_embedding_BC(9:30-11:30=>2h)

     从训练曲线来看，policy_embedding似乎还没有训练好？怎么解决？继续训练？

  2. 给耿导发PPT；

  3. 缩短训练周期：

     目前就两种途径，

     (1)减少迭代次数：减少num_iters=3e3（改成1500*8=>需要50\*8min=7h）；【OK】

     (2)减少每轮迭代的训练时间：

     timesteps_per_batch=int(2 ** 14)改成2 ** 8=256；【OK】

     注释cat训练和BC训练时的回显；

     注释policy train和discriminator阶段的evaluation（trpo_mpi.py 514-544和565-588）；【OK】

     (3)合并训练环节

     目前训练cat主要分6阶段：生成专家+加噪+训练cat+augmentation+BC；

     合并augmentation+BC为augBC得到：生成专家+加噪+训练cat+augBC；

     合并的好处就是不用保存augmentataion中间结果，最好改成aug一次就BC一次；

     但是存在一个问题，augmentataion和BC都调用了mlp_policy网络结构，里面的层名字同，产生冲突；

  4. 改policy_embedding

     (1)inverse module改为1层；

     

- 明日安排：

  1. 合并augmentation+BC为augBC;【OK】
  2. 改进policy_embedding，分析为什么;

### 周工作安排（11/29-12/3）
1. 给Noise Network加约束，跑实验；
2. BDM使用所有轨迹进行训练，而非只用buffer里的transition，因为我们只需要保证BDM的准确性；
3. 思考在CAT基础上改进，提取策略embedding（过滤表观embedding），作为discriminator的输入。
4. 目前主要工作：
   （1）跑mujoco（Ant,Hopper环境）实验；
   （2）实现离散空间（MiniGrid）环境实验；
   （3）尝试用RL的所有transition去训练BDM，而不只是buffer里的少部分；
   （4）在BDM增广时，约束轨迹长度、与出发点距离等；
   （5）尝试挖掘轨迹的决策信息（embedding）；
   两个Encoder，Enc(s,a)-Enc(s)=决策信息。

#### 2021/12/5

【余10天】

- 今日工作：

  1.跑实验所需时间太久了，感觉不能这么跑；

  policy_embedding的方法，目前要跑1.8天+增广1.6小时+BC训练和验证

  一次实验就至少需要2天时间，一周最多做两组实验，根本没办法做。

  目前首要问题：如何缩短训练周期；

- 明日安排：

  1. 给耿导发PPT；

  2. 做周计划；

  3. 和师兄讨论目前问题：

     （1）如何缩短训练周期；

     （2）policy_embedding结果。

#### 2021/12/4

【余11天】

- 今日工作：
  1. 合并cat+add-noise-uniform；
- 明日安排：

#### 2021/12/3

【余12天】

- 今日工作：

  1. 问王烨文毕设选题；

  2. 重跑cat**去除L2 loss**，服务器上没装pandas，so......

     Cat_without-l2-loss_HalfCheetach: BC_evaluation(209:1707839,Finished)

     Cat_with-l2-loss_HalfCheetach: BC_evaluation(209:1707469,Finished)

  3. 跑**noisenetwork_with-cov-l2loss**（209：1709114,终止了）重跑的实验，之前的可能被挤掉了。估计8号才能跑完；

     又停了，不知道为啥，先不跑这个了

  4. 写**policy_embedding**套入cat（跑起来了，大概要跑1.8天，估计5号才能跑完, 209:1937746）

     （4）generator的reward输入；【OK】

     （3）discriminator训练的输入（expert, agent）；【OK】

     （2）在discriminator的训练过程中加inverse model的训练；【OK】

     （1）补充inverse model（提取policy_embedding）网络结构；【OK】

     （5）expert-dataset, semi-dataset需要next_obs

     （6）discriminator的st不需要归一化，改成inverse module的st做归一化。

     （7）inverse module的网络命名需要改。（独立命名网络层）

     （8）跑**对比实验cat**(209:1938074)

- 明日安排：

  1. 做一下挑战杯PPT；
  2. 做一下党史学习教育5次线上测试；
  3. BC训练和评估合并；

#### 2021/12/2

【余13天】

- 今日工作：

  1. BC_CL_randomize-False_Evaluation_Hopper(Finished)

  2. cat**去除L2 loss**

     Cat_without-l2-loss_HalfCheetach: BC_evaluation(209:1562256)

     Cat_with-l2-loss_HalfCheetach: BC_evaluation(209:1561780)

  3. 跑**noisenetwork_with-cov-l2loss**（209：1562839）重跑的实验，之前的可能被挤掉了。估计7号才能跑完

  4. ICM参考代码

     [论文作者github:noreward_rl](https://github.com/pathak22/noreward-rl/projects)

     [curiosity-driven-exploration-pytorch](https://github.com/jcwleo/curiosity-driven-exploration-pytorch/blob/bacbefdfbdbc4c4382ab67147c9c8410305a4978/agents.py#L13)

     [Street-fighter-A3C-ICM-pytorch](https://github.com/uvipen/Street-fighter-A3C-ICM-pytorch/blob/master/src/process.py)

  5. 尝试写policy_embedding套入cat。

     （4）generator的reward输入；【OK】

     （3）discriminator训练的输入（expert, agent）；

     （2）在discriminator的训练过程中加inverse model的训练；

     （1）补充inverse model（提取policy_embedding）网络结构；【OK】

- 明日安排：

  1. 实现policy_embedding；【OK】
  2. 预约九价；【OK】

#### 2021/12/1

【余14天】

- 今日工作：

  1. 接种疫苗；

  2. 投票；

  3. 办居住证；

  4. 跑自适应cat_BC

     BC_Adaptation_vpred_Ant（Finished）

     BC_Adaptation_env_rew_Ant（Finished）

     BC_Ant（Finished）

  5. 跑**noisenetwork_with-cov-l2loss**（209：427619）重跑的实验，估计6号才能跑完；

     我好像跑错代码了，我把l2_loss定义为

     ```python
     l2_loss = (cov_diag ** 2).mean() if semi_dataset else 0
     ```

     此时目标为l2_loss尽可能小，也就是cov_diag尽可能小，

     而我应该希望cov_diag尽可能大，因此定义l2_loss改为

     ```python
     l2_loss = 0-(cov_diag ** 2).mean() if semi_dataset else 0
     ```

  6. cat**去除L2 loss**,估计明天（2号）跑完；

     Cat_without-l2-loss_HalfCheetach: BC(209:427310)

     Cat_with-l2-loss_HalfCheetach: BC(209:426955)

  7. 和师兄讨论policy_embedding方法，质疑：$\phi(s_t)$如何训练，比较难训.

     查看ICM如何训练的。我看其他人实现的ICM，其中inverse module的loss都是$CE(\hat{a}, a)$，并没有交替训练phi这个embedding.​​

- 明日安排：

  1. 找师兄要一下专利模板+问一下重复实验的问题；【OK】

  2. 讨论policy embedding如何训练的问题+自适应BC问题。【OK】

  3. 打电话问一下同仁医院九价预约材料。【OK】

  4. 尝试写policy_embedding套入cat看效果。

#### 2021/11/30

【余15天】

- 今日工作：

  1. 根据policy估计的价值/环境反馈rew(感觉不合理，因为IL的问题设定是没有rew)，自适应cat_BC.

  **BC_Adaptation_vpred_Ant**

  2. 观察Cat的w/o-l2-loss实验600轮结果；

  需要给noisenetwork的generator训练加约束，降低discriminator的acc；

  3. 跑BDM_Hopper：

  BC_CL_randomize-True_evaluation_Hopper(214:793668 Finished);

  **BC_CL_randomize-False_train_Hopper**(214:796664);估计12月2号跑完

  4. BDM使用所有轨迹进行训练，而非只用buffer里的transition，因为我们只需要保证BDM的准确性；

  但是我训练BDM的时候用的就是所有交互轨迹，只是没有加约束；

  5. 提交中期考核表+开题报告；
  6. 跑**noisenetwork_with-cov-l2loss**（209：1883399）

- 明日安排：

  1. 接种疫苗；【OK】
  2. 投票；【OK】
  3. 办居住证；【OK】
  4. 问王烨文毕设选题；
  5. 思考如果约束noise network输出，使得多样性越大越好；
  6. 参考ICM，提取policy_embedding套入cat看效果。

#### 2021/11/29

【余16天】

- 今日工作：

1. 写十九届六中全会心得；

2. 给于钧瑶寄证书；

3. 跑BDM_Hopper：BC_CL(214:699129);

4. cat**去除L2 loss**，看训练曲线是否与Noise Network类似。（Total Reward振荡变化，而非后期一直增加）

   Cat_without-l2-loss_HalfCheetach: cat(209:884774)

   Cat_with-l2-loss_HalfCheetach: cat(209:885128)

5. 搜索“augment”相关论文（ICLR2021，IMCL2021）

   没有找到有意思的，关于RL轨迹增广的。

- 明日安排：
  1. BDM使用所有轨迹进行训练，而非只用buffer里的transition，因为我们只需要保证BDM的准确性；
  2. 可视化Cat增广后的轨迹。variance?分布？
  3. Cat的w/o-l2-loss实验跑完后，思考如果约束noise network输出，使得多样性越大越好。
  4. 提交中期考核表+开题报告；【OK】
  5. 问一下王烨文比设选题；【OK】
  6. 根据discriminator得分，自适应cat_BC.【OK】

---
## 2021/11计划
1. 中期考核：
   (1)申优2021.11.16（录用证明，期刊等级证明，学生工作证明）;
   (2)学术活动卡2021.11.23（3次以上讲座记录）;
   (3)开题报告2021.11.30（双面打印，盖章后扫描上传系统）;
   (4)中期考核表2021.11.30（盖章后扫描上传系统）;
   
2. 11月15号分享论文。

3. 11月8号-12号院内评审。

4. 轨迹增广。

5. IJCAI会议结稿日期Sat Jan 15 2022 19:59:59 GMT+0800（2022/1/15）

   计划12月15日前实验做的差不多，12月16日开始写论文。


### 周工作安排（11/22-11/26）
1. 开题答辩（11/22）
周四下午三点半左右，准备5分钟左右的ppt汇报;
13号给老师看一下开题报告。
2. BDM其余环境跑实验（11/23）；
3. 看论文，提取策略信息embedding的文章;

#### 2021/11/25
【余21天】
- 今日工作：
1.开题答辩；
- 明日安排：
1.整理下一步研究思路 
2.跑mujoco（Ant,Hopper环境）实验；

#### 2021/11/24

【余22天】

- 今日工作：

  1.熟悉开题报告PPT；

  2.跑mujoco（Ant,Hopper环境）实验；

  (1)Ant:

  cat:Finished

  noise network:Finished

  BDM:Finished

  (2)Hopper:

  cat:Finished

  noise network:Finished

  BDM:run(2440645)

  3.尝试2个Encoder-Decoder。

  （1）generator的reward输入；

  （2）在discriminator的训练过程中加Encoder-Decoder的训练；

  （3）补充Encoder-Decoder网络结构(20%)；

  参考代码

  [DomainIntersectionDifference](https://github.com/sagiebenaim/DomainIntersectionDifference)

  [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)

  [Building Autoencoders in Keras中文文档](https://keras-cn.readthedocs.io/en/latest/legacy/blog/autoencoder/)

  4.Keras词嵌入

  （1）skip-gram（NLP的一种方法）根据中心词预测周围词。

  联想到轨迹具有先天的时序信息->蕴含策略信息+状态转移函数信息+MDP信息，可以通过顺序构建样本对，正序->1，反序->0，进行自监督训练学表示。（但是时序中蕴含策略信息）

- 明日安排：

  1.熟悉开题报告PPT；

  2.跑mujoco（Ant,Hopper环境）实验；

  3.尝试2个Encoder-Decoder。

  （1）generator的reward输入；

  （2）在discriminator的训练过程中加Encoder-Decoder的训练；

  （3）补充Encoder-Decoder网络结构(20%)；

#### 2021/11/23

【余23天】

- 今日工作：

  1.熟悉开题报告PPT；

  2.跑mujoco（Ant,Hopper环境）实验；

  (1)Ant:

  cat:BC

  noise network:augment

  BDM:BC_CL_randomize-True

  (2)Hopper:

  cat:BC(2594692)

  noise network:augment(2598679)

  BDM:run(2440645)

  3.Domain Intersection and Domain Difference

  [知乎Domain Intersection and Domain Difference](https://zhuanlan.zhihu.com/p/95620835)

- 明日安排：

  1.熟悉开题报告PPT；

  2.跑mujoco（Ant,Hopper环境）实验；

  3.尝试2个Encoder-Decoder。

  参考[github](https://github.com/sagiebenaim/DomainIntersectionDifference/blob/13a492d72bbeb1471b158a6488505d50ed718b49/train/train.py#L14)

#### 2021/11/22

【余24天】
- 今日工作：
1. 调整开题报告PPT；
2. 交讲座记录卡；
3. 目前主要工作：
   （1）跑mujoco（Ant,Hopper环境）实验；
   （2）实现离散空间（MiniGrid）环境实验；
   （3）尝试用RL的所有transition去训练BDM，而不只是buffer里的少部分；
   （4）在BDM增广时，约束轨迹长度、与出发点距离等；
   （5）尝试挖掘轨迹的决策信息（embedding）；
   两个Encoder，Enc(s,a)-Enc(s)=决策信息。
- 明日安排：
1. 熟悉开题报告PPT；
2. 跑mujoco（Ant,Hopper环境）实验；
3. 尝试两个Encoder，Enc(s,a)-Enc(s)=决策信息。
### 周工作安排（11/15-11/19）

1.中期考核： 
（1）找老师讨论开题报告题目(11/19)。

2.准备论文PPT（11/16）

3.准备开题答辩PPT（11/18，11/21）

4.挑战杯申报书（11/19-11/20）
#### 2021/11/19
【余26天】
- 今日工作：
1. 目前主要工作：
（1）开题报告答辩PPT；（19号，21号）
（2）挑战杯申报书；（20号）
（3）更换mujoco环境跑cat+离散环境跑BDM；
#### 2021/11/18
【余27天】
- 今日工作：

1.熟悉PPT;

2.BDM与GAIL类似
  区别在与GAIL（s->a），而BDM是(st+1->(st, at)).
  gail会递归的进行（s->a）吗？如果不会，那么我们也可以只进行一次反向生成训练，不递归。

#### 2021/11/17
【余28天】
- 今日工作：

1.完善组会PPT+理顺；

2.Goal GAN用了Least-Squares GAN(discriminator$\sim[-1,1]$，generator希望discriminator判别不出来，即输出0)
  感觉挺合理的，获取可以试试。
  但是感觉没啥区别，估计差不多。

3.从课程角度来说，不补全增广，然后all_CL训练也是合理的。
  可以尝试一下。
  估计不使用专家效果不会好。

4.从Goal GAN的反思，里面$R^g(\pi _i)\sim(R_{min}, R_{max})$的设计促成了课程goal的实现。
  BDM实验时，这种课程的约束感觉也是一个trick。

5.极大似然估计等价于
  （1）回归问题：最小二乘法（最小化均方误差损失）；
  （2）分类问题：最小化交叉熵损失；
  【参考】[极大似然函数、最小二乘、交叉熵之间的联系](https://zhuanlan.zhihu.com/p/39775467)

6.可以尝试再换一些mujoco环境（稀疏奖赏的环境）；

7.可以尝试BDM训练的时候只做一次反向增广。
  似乎又不太合理，Backtracking Model是因为用极大似然估计去学习的，等价于回归问题。

8.感觉BDM更适用于导航类的task，相较于HalfCheetach这类重复性task而言？

9.Noise Network和mixup感觉是弱增广，BDM属于强增广。
  应该发挥BDM在生成难样本上的优势。

10.Recall Traces方法的backtracking model增广长度是一个超参数。
  我的BDM则是增广到原始长度。可以尝试一下控制增广长度，因为越长越不可控。
  但是不增广到同等长度或更长，那是不是意味着不是最优、次优轨迹。因为长度<专家轨迹长度。

11.优先经验回放（Prioritized Experience Replay, PER）
  RL为了提高sample efficiency，通过Importance Sample来利用以往的经验，这也是replay buffer的来源。
  以往Experience Replay都是从replay buffer里均匀采样进行训练，二PER则是根据优先级，采样那些优先级高的经验用于训练。
  [论文笔记7：Prioritized Experience Replay](https://zhuanlan.zhihu.com/p/38358183)
#### 2021/11/16

- 今日工作：

1.214服务器上的代码跑完了：
  （1）all_HalfCheetah-v2_d-lr-4e-4_g-lr-1e-4_BDM；
  （2）all_HalfCheetah-v2_label-smoothing_d-lr-4e-4_g-lr-1e-4_BDM；
  （3）all_HalfCheetah-v2_label-smoothing_BDM；

2.准备组会分享PPT;

（1）cat训练不需要与环境交互；

（2）noise network的ac_new需要与环境交互；

（3）expert-7、noise network(lambda=0.9，hour001 )的增广avgret；

noise network(lambda=0.9，hour001 )、BDM_CL的BC avgret.

- 明日安排： 
1.完善组会PPT+理顺；
2.做开题答辩PPT。

#### 2021/11/15

- 今日工作：

  1.补充实验：
  （1）不补全轨迹+7条专家轨迹；

  （2）补全轨迹+无专家+BC_max_iters=150000；

  增加了randomize

  2.改backward dynamic代码，在离散环境（MiniGrid）做BDM对比实验；

  （1）模型输入：act->one-hot编码；

  （2）模型输出：one-hot编码->act；

  （3）环境；

  （4）agent需要换成RNN训练，因为环境obs为(56, 56, 3)；

  因此agent输出$(\Delta s_t, a_t)$是个问题。

  3.跑一下所有noisenetwork_Delta_st_at中间模型的BC结果，说不定有比得过cat的。

- 明日安排：

###  周工作安排（11/8-11/12）

1. 中期考核： 
   （1）找老师讨论开题报告题目(11/15)。
2. 准备论文PPT（11/9-11/11）
3. 尝试反向增广轨迹+正向动力学验证

#### 2021/11/14

- 今日工作：

  1.补全BDM轨迹;

  2.all模型课程训练BC

  BC验证结果基本avg_ret都为负数，现在补充不补全轨迹+7条专家轨迹。如果这个实验avg_ret也基本为负数，那说明我修改后的backward_augment_trajectories.py有问题。如果这个实验avg_ret与之前的all实验结果差不多，那就说明补全的轨迹有问题。

- 明日安排：

  1.改backward dynamic代码，在离散环境（MiniGrid）做BDM对比实验；

  2.看GoalGAN论文。

#### 2021/11/13

- 今日工作：

  1.我下一步主要工作：

  （1）补全BDM轨迹再做一次对比；

  （2）all模型课程训练BC；

  对于all模型增广出来的轨迹，提前根据增广长度（aug_len, 不加补全部分）进行排序。

  再按照该顺序训练BC。

  a. 类似[curriculum-learning-for-deep-learning](https://github.com/peace195/curriculum-learning-for-deep-learning)根据agu_len排序增广轨迹，分成三阶段（head, middle, tail），每个阶段训练收敛再到下一个阶段。

  （3）改backward dynamic代码，在离散环境（MiniGrid）做BDM对比实验；

  2.在Hopper-v2, Ant-v2上训练BDM（all）。

- 明日安排：

#### 2021/11/12

- 今日工作：

  1.在Hopper-v2上做all实验；

  2.看GoalGAN论文(25%);

  3.尝试补全轨迹重新做backward_dynamic（HalfCheetah_ext-v2）

  保存增广轨迹长度，课程学习时可能要用。

  4.用最优generator跑课程学习（感觉head/middle/tail不太好控制，能不能直接用all模型做课程学习）。

  因为我的all模型都跑过了，可以找到最好的generator，好控制。

  5.处理GAN训练不稳定性[参考博客](https://bbs.cvmart.net/articles/4253)

  （1）加噪：给expert_data, generate_data加噪然后再输入discriminator训练；

  （2）标签平滑：将expert_data的label由1->0.9，generate_data的label由0->0.1；

  ```python
  # generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=generator_logits, labels=tf.zeros_like(generator_logits))
  generator_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=generator_logits, labels=tf.add(tf.zeros_like(generator_logits), 0.1))
  # expert_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=expert_logits, labels=tf.ones_like(expert_logits))
  expert_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=expert_logits, labels=tf.add(tf.ones_like(expert_logits),-0.1))
  ```

  （3）调整generator和discriminator学习率：d_lr（0.0004） > g_lr（0.0001）；

  原来：d_lr(3e-4=0.0003) < g_lr(1e-3=0.001)

  6.GAN的收敛转瞬即逝，不会达到一个稳态。

  所以从discriminator的accuracy->0.5来判断训练是否收敛是有道理的，现在的问题是我的GAN直接就训练的discriminator的accuracy->1，generator的Reward->0，generator根本就没有训练到东西。是模式崩塌吗？

  我希望看到discriminator的accuracy先上升->再下降->再上升这样的过程，我取下降时候的generator应该就是收敛的。现在的问题是并未观察到先上升->再下降->再上升这种趋势。

  考虑是否要增加BDM最大训练iters，因为我看HalfCheetah-v2这个环境的all模型Reward后面在增加，从BC验证结果来看倒数第二个模型性能最好。

- 明日安排：

#### 2021/11/11

- 今日工作：

  1.找最优的generator。

  现在的做法是对all实验，对所有保存模型去做BC，看性能，对比expert_acc/Reward看能不能观察出一些最优generator规律。

  2.尝试做离散空间的实验+换一个Mujoco环境做实验。

  3.看GoalGAN论文（20%）；

- 明日安排：

  1.GoalGAN如何工作。

  2.尝试做离散空间的实验+换一个Mujoco环境做实验。

  3.GAIL理论推导，结合Recall Traces里的变分解释可以做理论推导。

#### 2021/11/10

- 今日工作：

  1.对比（类似课程学习）：

  (1)head; (2)middle; (3)tail; (4)all; (5)head->middle->tail有序训练BC。

  2.课程学习如何训练？

  权重动态分配，可以尝试用增广轨迹长度的倒数作为权重，在此基础上随着训练论述的增加动态调整权重。

  3.对比expert-7进行重复71次的结果。

  4.发现all这个设置：hour009/iters_835的模型训练BC得到的结果比hour035/iters_2961的好。

  说明之前实验并未挑选最好的generator。

  现在的问题就是如何挑选最优generator，暴力解决吗？时间代价太大，而且也需要BC验证（与环境交互代价大）。暴力挑选其实有点作弊，因为专家轨迹少的原因就是交互代价大，现在为了挑选最优generator还需要与环境交互，违背了问题设定。

  有没有好办法能挑选出最优的generator。

- 明日安排：

  1.GoalGAN如何工作。

  2.找最有的generator。

  重新用最优generator（cat，noisenetwork，backward dynamic model）去增广数据，进行对比实验。

  3.GAIL理论推导，结合Recall Traces里的变分解释可以做理论推导。

#### 2021/11/9

- 今日工作：

  1.如何判断GAN收敛，即何时停止GAN的训练，用什么时候的generator增广轨迹最好？

  根据博客（https://blog.csdn.net/u014281392/article/details/111934100），GAN训练到最优时，达到纳什均衡，$P_{Gen}(x^*)\approx P_{True}(x), D^*(x)=0.5$​，即生成器样本分布与真实分布同，而判别器无法判断样本来源。

  因此我应该用D(x)=0.5时刻的Generator做增广吗？​

  2.尝试补全反向增广轨迹做BC；尝试用D(x)=0.5时刻的Generator做增广。

  3.思考Recall Traces论文：

  （1）backward dynamic model最大似然训练是怎么训练的；

  - Recall Traces中backtracking model的训练：

  给定RL算法与环境交互的transition $(s_t, a_t, r_t,s_{t+1}, done)$​​​，用bw_actgen得到概率$p(a_t|s_{t+1})$​​​，$loss_{actgen} = -log p(a_t|s_{t+1})+weight1 * entropy(p(*|s_{t+1}))$​​。然后用bw_stategen得到$p(\Delta \hat{s_t}|s_{t+1},a_t)$​​，$loss_{stategen} = MSE(\Delta \hat{s_t}, s_t-s_{t+1})$​​​​​。$loss_{total}=loss_{actgen}+weight2*loss_{stategen}$。

  Recall Traces训练的时候只做了一步反向增广，且都是从专家状态->专家状态动作对。（本质都是监督模式的学习，感觉和BC类似，应该也会出现composed error问题）

  - backtracking model生成Recall Traces做模仿学习：

  从专家轨迹里采样一个专家状态作为出发点，然后用模型backtracking model递归进行增广，然后用于BC。

  - 我们的方法与此不同，我们用GAN训练DBM：

  给定RL算法与环境交互的transition $(s_t, a_t, r_t,s_{t+1}, done)$​，用BDM得到$p(\Delta \hat{s_t}, \hat{a_t} | s_{t+1})$​，再从$\hat{s_t}=s_{t+1}+\Delta \hat{s_t}$​递归展开反向轨迹（$p(\Delta \hat{s_{t-1}}, \hat{a_{t-1}} | \hat{s_t})$​）。

  （2）优化目标的变分解释；

  - 通过变分推断，得到ELBO+KL形式；（RL model: $\theta$​， backtracking model: $\phi$）

  $$
  \begin{align*}\label{2}
  & p(R>L|\theta)=\frac{p(R>L,\tau|\theta)}{p(\tau|R>L,\theta)}\\
  & \log p(R>L|\theta)=\log\frac{p(R>L,\tau|\theta)}{q(\tau)}-\log\frac{p(\tau|R>L,\theta)}{q(\tau)}
  \end{align*}
  $$

  $$
  \text{左边}=\int {q(\tau)\log p(R>L|\theta)} \,{\rm d}\tau=\log p(R>L|\theta)
  $$

  $$
  \text{右边}=\int {q(\tau)\log\frac{p(R>L,\tau|\theta)}{q(\tau)}}\,{\rm d}\tau-\int {q(\tau)\log\frac{p(\tau|R>L,\theta)}{q(\tau)}}\,{\rm d}\tau\\
  =\text{ELBO}+\text{KL}(q(\tau)||p(\tau|R>L,\theta))
  $$

  $$
  \therefore \log p(R>L|\theta)=\text{ELBO}+\text{KL}(q(\tau)||p(\tau|R>L,\theta))\\
  \geq\text{ELBO}
  $$

  - 使用EM算法优化目标函数（通过最大化ELBO来最大化$\log p(R>L|\theta)$）；

  a. E-step：固定$\theta$​​，通过最小化KL来最大化ELBO
  $$
  \phi=\underset{\phi}{argmin\,} \text{KL}(q_\phi(\tau)||p(\tau|R>L,\theta))\\
  =\underset{\phi}{argmin\,} \text{KL}(p(\tau|R>L,\theta)||q_\phi(\tau))
  $$

  $$
  \because \text{KL}(p(\tau|R>L,\theta)||q_\phi(\tau))=\int{p(\tau|R>L,\theta)\log\frac{p(\tau|R>L,\theta)}{q_\phi(\tau)}}\,{\rm d}\tau \\
  =\int{p(\tau|R>L,\theta)\log p(\tau|R>L,\theta)\,{\rm d}\tau-\int{p(\tau|R>L,\theta)\log q_\phi(\tau)}\,{\rm d}\tau}
  $$

  $$
  \therefore \phi = \underset{\phi}{argmax\,} \int{p(\tau|R>L,\theta)\log q_\phi(\tau)}\,{\rm d}\tau\\
  =\underset{\phi}{argmax\,} E_{\tau \sim p(\tau|R>L,\theta)}[\log q_\phi(\tau)]
  $$

  b. M-step：固定$\phi$，最大化ELBO
  $$
  \text{ELBO}=\int {q_{\phi}(\tau)\log\frac{p(R>L,\tau|\theta)}{q_{\phi}(\tau)}}\,{\rm d}\tau\\
  =\int {q_{\phi}(\tau)\log p(R>L,\tau|\theta)}\,{\rm d}\tau - \int {q_{\phi}(\tau)\log q_{\phi}(\tau)}\,{\rm d}\tau
  $$

  $$
  \therefore \theta=\underset{\theta}{argmax\,}\text{ELBO}
  =\underset{\theta}{argmax\,}\int {q_{\phi}(\tau)\log p(R>L,\tau|\theta)}\,{\rm d}\tau
  $$

  $$
  \because q_{\phi}(\tau)=p(\tau|R>L,\theta_t)
  $$

  $$
  \therefore \theta=\underset{\theta}{argmax\,}E_{\tau\sim q_{\phi}(\tau)}[\log p(R>L,\tau|\theta)]\\
  =\underset{\theta}{argmax\,}E_{\tau\sim q_{\phi}(\tau)}[\log p(\tau|\theta)]
  $$

  （3）GoalGAN如何工作。

  4.Recall Traces代码里似乎是处理的离散动作空间啊～

  ```python
  # bw_module.py line151
  action_log_probs, dist_entropy = evaluate_actions_sil(pi, actions)
  ```

  其实从他的论文推理来说也可以看出来是离散空间，连续空间应该无法求$p(a_t|s_{t+1})$​​​这个概率值。

  5.EM算法

  ![image-20211109224847743](/home/hehua/.config/Typora/typora-user-images/image-20211109224847743.png)

  [参考知乎：EM](https://zhuanlan.zhihu.com/p/78311644)

  6.最小化KL散度，相当于最大化似然。

  [参考博客](https://gitpress.io/@jwzhang/KL-divergence)

- 明日安排：

  1.对比（类似课程学习）：

  (1)head; (2)middle; (3)tail; (4)all; (5)all+有序训练BC(head->middle->tail); (6)head->middle->tail有序训练BC。

  2.尝试补全反向增广轨迹做BC。

  3.准备候选人竞选发言（导师/党龄/以往职位......）。

  4.GoalGAN如何工作。

#### 2021/11/8

- 今日工作：

  1.暂时不清楚GAN(或者GAIL的生成器/判别器的loss变化,或者什么手段判断训练好了);

  [GAN训练问题](https://zhuanlan.zhihu.com/p/56943597)

  2.考虑到backward dynamic model都是从专家轨迹倒数(1/3)范围内的状态开始增广,因此在BC训练阶段是需要把专家轨迹放进去的.

  发现放进去后，训练效果提高了。

  3.反向增广，由于给定的专家轨迹是有限的，因此discriminator有bias。有可能向discriminator中引入先验知识或其他知识吗？

  4.cat无法用于离散动作空间，noisenetwork也不可以，backward dynamic mode了可以。

  是不是可以尝试在MiniGrid环境上训练。

  5.如果限制反向增广轨迹长度，那么可以不用限制初始状态为专家轨迹的后三分之一历经状态。

  6.考虑分三阶段训练backward dynamic model（前1/3，中1/3，后1/3）。

  对比实验时，可以对比：

  (1)head; (2)middle; (3)tail; (4)all; (5)all+有序训练BC(head->middle->tail); (6)head->middle->tail有序训练BC。

  7.补全反向增广轨迹做BC，与cat相比感觉才公平。

  8.为什么反向增广实验，训练时iter=5（或其他）avgret固定了？

  检查发现，iter=5，sampling就不输出了，发现最后一次增广aug_traj_len=0，尝试begin不从0开始。

  这样的话,head实验也会有这个问题，而middle，tail没有这个问题。

  9.Recall Traces附录里描述了normalized state，并对增广后的$s_t, a_t$​进行un-normalize。

  cat代码里policy对状态（$s_{t+1}$​​）做了normalize，并没有un-normalize，感觉没有必要啊.

  10.Recall Traces利用最大化似然的方法来训练backward dynamic model，这怎么训练？直接将对数似然求梯度训练？

  [结合代码理解](https://github.com/soumye/recalltraces)

  11.Recall Traces里的变分解释不是很理解，为什么要解释这个，和文中提出backward dynamic model有什么联系吗？

  [结合知乎理解，Recall Traces：基于反向模型的强化学习](https://zhuanlan.zhihu.com/p/109924536)

- 明日安排:

  1.对比（类似课程学习）：

  (1)head; (2)middle; (3)tail; (4)all; (5)all+有序训练BC(head->middle->tail); (6)head->middle->tail有序训练BC。

  2.尝试补全反向增广轨迹做BC。

### 周工作安排（11/1-11/5）
1. 中期考核： 
   （1）录用证明，期刊等级证明，学生工作证明（11/5）；
   （2）找老师讨论开题报告题目(11/5)。
2. 准备论文PPT（11/2-11/4）
3. 轨迹增广尝试不同融合方法

#### 2021/11/7

- 今日工作：

  1.Recall Traces里面反向增广轨迹长度到底选择的多少,正如文中所说,太常了会偏移,导致增广的状态不是inital state所能到达的状态.

  ```txt
   the length is adjusted manually based on the time-scale of each task
  ```

  难道是超参?需要人为调整?

#### 2021/11/6

- 今日工作：

  1.$q_\phi(s_t, a_t | s_{t+1})$实验效果不好,avgret为负数.

  感觉BDM模型没有训练好,但是已经训练很久了,说明收敛速度慢.

  2.跑$q_\phi(\Delta s_t, a_t | s_{t+1})$​

- 明日安排：

  1.看Recall traces论文；

  2.跑Recall traces代码；

#### 2021/11/5

- 今日工作：

  1.看Recall traces论文；

#### 2021/11/4

- 今日工作：

  1.发现backward dynamic model训练时，若不做正向动力学验证(last_ob, last_ac)->curr_ob，那么这种增广方法就不需要与环境交互，能提高训练效率；

  2.如果不用正向动力学(last_ob, last_ac)->curr_ob去约束backward dynamic model的训练，可以暂时用(last_ob, last_ac)与真实的(true_last_ob, true_last_ac)的二范数来约束。

  但是感觉还是不合理，如果不用正向动力学验证，很难保证生成的轨迹是符合马尔科夫性的。

  测试了一下backward dynamic model(hour010的iter_950)，效果非常不好。

  ```bash
  Average length: 1000.0
  Average return: -706.4406595900199
  number of trajectories: 100
  success policy total = 0.0  out of 100
  success policy percentage = 0.0%
  ```

  3.后面可以尝试：

  （1）专家轨迹应该用于训练；

  （2）补全反向生成轨迹的后半段（就不需要直接用专家轨迹训练了）；

  （3）backward dynamic model输出的action做clip，应该是动作超出动作空间，才导致reward为负数。需要约束last_ac, last_ob；

  （4）根据recall traces的trick，预测状态的差。$q_\phi(\Delta s_t, a_t | s_{t+1})$，recall traces把联合分布做了分解$q_\phi(\Delta s_t, a_t | s_{t+1})=q(\Delta s_t | a_t, s_{t+1})q(a_t | s_{t+1})$，考虑我要不要分解，分解是不是会更好？好在哪里？

  4.为什么backward dynamic model的Total Reward能增加到2000+？代码是不是有问题。

  没问题，discriminator的预测$y\in(0,1)$，而$reward=-log(1-y+10^{-8})\in(log\frac{1}{1+10^{-8}}, log\frac{1}{10^{-8}})$​，即$reward\in(-4\times10^{-9},8)$​。

  考虑到最长轨迹长度为1000，所以累积reward应属于$(-4\times 10^{-6},8000)$，而$2000\in(-4\times 10^{-6},8000)$​，合理。​

  但是，难道他要到8000多才收敛吗？收敛慢的原因估计是因为我没有对输出的(last_ob, last_ac)做约束（比如引入先验：ac范围等等）。

- 明日安排：

  1.看Recall traces论文；

  2.跑Recall traces代码；

  3.PlayVirtual如何实现循环一致训练，和我们方法区别。

#### 2021/11/3

- 今日工作：

  1.分析mixup实验现象；

  2.avgret为什么会有负的，什么情况下会出现？

  3.写GAN训练反向动力学模型代码。

  需要修改两个地方，（1）dataset的batch生成；（2）generator的input, output维度。（3）输入discriminator的(s,a)

  问题：

  （1）反向动力学模型输出(last_ob, last_ac)，暂时无法把last_ob设置到环境，进行正向动力学验证(last_ob, last_ac)->curr_ob。

  4.找到recall traces的代码：https://github.com/soumye/recalltraces

- 明日安排：

  1.约束反向动力学模型的训练（正向动力学模型，循环一致性）；

  2.结合Recall traces论文看代码；

#### 2021/11/2

- 今日工作：

  1.下流BC任务跑起来；

  （1）7条专家轨迹+BC【OK】；

  （2）cat矫正7\*71+BC【OK】；

  （3）mu_0_cov_diag_weight_lambda加噪7*71+BC【OK】；

  2.对比cov_weight_lambda方法增广后的轨迹BC与直接用专家轨迹BC效果；

  cov_weight_lambda方法好，说明我们的增广方法有效果，但是没有cat好；

  3.mixup任意两条专家轨迹mixup用于BC训练:

  lambda=0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9;

  alpha=0.1, 0.2, 0.5, 1, 2, 3, 5, 10, 20, 50;

- 明日安排：

  1.找师兄讨论实验结果；

  2.确定下一步实验（可以结合反向动力学模型）；

  3.看反向动力学模型论文，做PPT；

#### 2021/11/1

- 今日工作：

  1.使用tensorflow-gpu==2.4跑代码发现，GPU数据传输比CPU慢。查到有人说GPU擅长做矩阵运算，对于小模型来说，GPU体现不出优势。

  so 跑代码还是换成cpu跑。

  2.跑mixup+mu_0_cov_diag(alpha=1)

  3.发现mu_0_cov_diag采样出来的noise仍然有超过[-1,1]范围的数据，为什么？多元高斯分布，保证cov在[0,1]，不能保证分布在[-1,1]吗？

  是的，不能，正态分布取值(负无穷，正无穷)，只是说在-1.96～+1.96范围内曲线下的面积等于0.9500，采样概率大。[参考：https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/552653]

  可是cat也没有限制输出范围在[-1,1]啊～

  4.发现原来将cov->sigmoid，1/（1+exp(-cov)）写成了1/（1+exp(cov)），单调性弄错了，但是应该没什么影响。

- 明日安排：

  1.下流模仿学习任务跑起来；

  2.对比cov_weight_lambda方法增广后的轨迹BC与直接用专家轨迹BC效果；

  如果cov_weight_lambda方法好，说明我们的增广方法有效果；

  3.尝试mixup任意两条专家轨迹(lambda=0.1, 0.2, 0.5, 1, 2, 3)用于BC训练，应该挺靠谱的；

  如果效果好，可以去学lambda（优化lambda使得模型性能提升最大）。

  4.联系戴老师0731-87000367，提供录用证明；

  5.期刊等级证明；

---
## 2021/10计划
1. 增强轨迹：实现DAugGI_eta1和DAugGI_eta2。
2. 修改论文第二稿；
    截至时间：2021-10-20；
3. 11月15号分享论文。
4. 11月8号-12号院内评审。

### 周工作安排（10/27-10/29）
1. 修改终审论文（10/28）【OK】；
   稿件计划修回时间：2021-11-14
2. 写开题报告【OK】；
3. 提交中期评定表（10/27）【OK】；
4. 申请特别奖学金（10/27）【OK】；
5. 做好党支部换届表决票（10/27）【OK】；

#### 2021/10/31
- 今日工作：
1. 开题报告；
2. 样板支部申报书；
- 明日安排：
1. 联系戴老师0731-87000367，提供录用证明；
2. 期刊等级证明；
3. 轨迹增广：
   （1）下流模仿学习任务跑起来；
   （2）尝试增加均匀噪声，对比效果。

#### 2021/10/29
- 今日工作：
1. 写开题报告：研究方案+可行性分析；

#### 2021/10/28
- 今日工作：
1. 打印中期评定表；
2. 写实践课程考核表+老师签字；
3. 邮寄保密审查材料；
4. 写开题报告（40%）
- 明日安排：
1. 写开题报告；
2. 参加党支部换届。

#### 2021/10/27
- 今日工作：
1. 制作党支部换届表决票；
2. 向编辑部反映最新稿件错误问题;
3. 申请特别奖学金；

- 明日安排：
1. 打印中期评定表；
2. 写实践课程考核表+老师签字；


### 周工作安排（10/18-10/22）

1.跑完整的cat-dauggi对比实验；

2.看论文找信息量和多样性的思路。

2021/10/22

- 今日工作：

  1.测试轨迹长度都是1000？

  是的，没有最终的done设置，只限定了最大轨迹长度。

  2.模型输出noise太大？

  感觉是的，action space：Box(-1,1,(6,),float32)，而noise是从均值为0，方差为模型输出的高斯分布中采样得到。也是在0.7左右的数量级。

  同时加噪后没有做裁减，导致action超过[-1,1]范围。

  3.观察action范围对加噪后action做裁减，比较cat noise的数量级与我的模型输出noise数量级。

  4.可以继续尝试一下original+clip/weight_lambda。

  5.感觉模型输出cov做sigmoid处理是不太合理的，因为大部分都会使得cov=-1/1，只有少部分在（-1，1）间。

- 明日安排：

2021/10/21

- 今日工作：

  1.分析noise network不好的原因可能是模型输出cov太大，导致noise太大，需要对比以下我生成的noise与cat的noise数量级。

  可以修改semi loss的权重为0.01，0.001试试。

- 明日安排：

  1.写开题报告。

2021/10/19

- 今日工作：

  1.stochastic_policy=False.

  2.服务器装tf2.1:

  ```python
  pip install tensorflow-gpu==2.1
  conda install cudnn=7.6 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/
  ```

  3.发现semi_path: ../data/HalfCheetah-v2/HalfCheetah-v2_expert_50_avgreturn-2437.5422168857763.npz可能存在问题

  即使semi_path处也处理为只使用7条轨迹，但是self.semilabels没有处理，仍然是50条轨迹，所以补充处理self.semilabels为 traj_limitation=7条。

  4.训练mujoco不需要filter(0,1)过滤不成功的轨迹，因为对于论文中的结果（0.9+success）来说都是很好的轨迹。

  而对于 Manipulation任务需要filter，还需要BC预训练10000步来加速gail的学习。

- 明日安排：

2021/10/18

- 今日工作：

  1.发现跑的实验，actor没有收敛，尝试增加专家轨迹数看看能不能收敛。

- 明日安排：

### 周工作安排（10/11-10/15）

1.实现想法：CAT-eta，RNN编码器；

2.看论文；

3.实验SQIL-eta：
（1）在不同稀疏奖赏环境下的效果；

（2）SQIL文章实验下的效果。

分别实验1/5/10/50条专家轨迹效果。

4.思考在CAT基础上如何做自监督：

（1）在CAT内部添加什么信息；

（2）在自监督框架内，CAT框架外添加什么信息；

（3）考虑是否增加Oracle信息（比如：对生成的轨迹做判断，轨迹是好是坏）

5.准备支部共建

（1）想一个主题；

（2）参会10个人；

（3）找3个人做七一讲话精神的学习总结（分享）；（10月15号）

（4）准备PPT；

（5）最后写一个新闻稿。

2021/10/17

- 今日工作：

  1.准备“七一”讲话精神学习分享PPT+讲稿；

  2.修改baselines代码用于tensorflow-gpu=2.4：1,3,5,7,9,11,12

  (1) /home/hehua/下载/baselines/baselines/common/misc_util.py

  ```python
  tf.random.set_seed(myseed)# 2=========================================20211017 20:28
  ```

  (2)/home/hehua/下载/baselines/baselines/common/mpi_running_mean_std.py

  ```python
  import tensorflow.compat.v1 as tf, baselines.common.tf_util as U, numpy as np# 4=====================20211017 20:28
  ```

  (3) /home/hehua/下载/baselines/baselines/common/tf_util.py

  ```python
  import tensorflow.compat.v1 as tf # pylint: ignore-module 6=========================================20211017 20:28
  ```

  (4) /home/hehua/下载/baselines/baselines/common/distributions.py

  ```python
  return self.mean + self.std * tf.random.normal(tf.shape(self.mean))# 8=========================================20211017 20:28
  ```

  ```python
  import tensorflow.compat.v1 as tf# 10=========================================20211017 20:28
  ```

- 明日安排：

  1.跑GAIL；

  2.查看noisenetwork输出的方差是否最后都为0了。

2021/10/15

- 今日工作：

  1.修改214密码SX2016056；

  2.注释214密钥。

  3.整理了cat_dauggi日志变量含义。

  4.改noise network输出为正太分布对角协方差的对角元。（$-\lambda||cov||_2^2$）

- 明日安排：

  1.跑gail

  2.解决gpu问题

  3.做七一讲话学习总结PPT。

2021/10/14

- 今日工作：

  1.改代码为noise network，在209卡1上跑起来了；

  这个也被挤掉了，在本地跑起来了。估计明天也跑不完。

  2.209卡0上的cat训好了似乎。

  ~_~似乎没有训好，可能是被挤掉了。

  3.CAT如何保存的模型；【OK】

  4.如何验证CAT；【OK】

- 明日安排：

  1.训练gail。

  2.改成GPU跑代码

#### 2021/10/13

- 今日工作：

  DAugGI代码跑起来了～～～～

- 明日安排：

  1.修改代码为noise network

#### 2021/10/12

- 今日工作：

  1.修改代码：DAugGI_NoiseNetwork。

  发现了一个巨坑，不停的填坑，最后发现没办法再动代码了，牵一发而动全身。唉～

  2.不同环境专家轨迹要求数量；

  基本都在1-50之间（参考CAT就行）。

- 明日安排：

  1.重写CAT代码

#### 2021/10/11

- 今日工作：

  1.CAT训练：

  （1）太慢：48+14=62h，跑到890个episode，平均14.35episode/h。因此，需要83.63h跑到1200个episode，仍需要21.63h，大概在12日早上6点左右跑完。

  （2）生成轨迹效果提升慢：论文中说明使用的专家轨迹为25条，

  2.查看CAT生成的action序列是否要与环境交互；

  查看原文发现：CAT中加噪后的action需要与环境交互，矫正后的轨迹不需要与环境交互（discriminator的第二项起到了作用：$||a-a'||_2^2$​​）。

- 明日安排：

  1.重置CAT:用25条专家轨迹作GAIL训练。

  2.不同环境专家轨迹要求数量；

  3.修改矫正网络为干扰网络。

  保证增广轨迹的语义信息后，增加信息量和多样性。

### 周工作安排（10/4-10/8）
1. 修改论文第二稿 初稿（10/7）；
2. 实现CAT_eta1和CAT_eta2（10/7-10/9）

#### 2021/10/8

- 今日工作：

  1.实现CAT;

  2.Mujoco稀疏环境：

  (1)**Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards** 

  当agent走到目标距离λ时，才收到+1的reward：SparseHopper-V2 (λ = 3), SparseHalfCheetah-V2 (λ = 15),Sparse Ant-V2 (λ = 3).

  (2)

  Four Rooms Navigation, Three-link Reacher, Fetch Push and Fetch Pick And Place 

- 明日安排：

  1.实现DAugGI；

#### 2021/10/7

- 今日工作：
1. 修改论文；
- 明日安排：
1. 实现CAT；

#### 2021/10/6
- 今日工作：
1. 看收集的文章。
ALIL;
- 明日安排：
1. 看收集的文章。
2. 尝试改论文；

#### 2021/10/5
- 今日工作：
1. 收集近2018-主动学习文章；
- 明日安排：
1. 看收集的文章。

#### 2021/10/4
- 今日工作：
1. 修改论文 
服务器上跑的代码被挤的跑了3天没跑完~.~乌鱼子
目前14min跑6次，平均2.33min/次，至少还需要605.8min约等于10h，今天晚上能跑完。
2. 写报告（SQIL-eta, MiniGrid-16x16）
- 明日安排：
1. 修改论文；
近三年主动学习的文献调研和分析。
---
## 2021/9计划
1. 增强轨迹;【20%OK】
结合自己想法修改Adversarial Imitation Learning with Trajectorial Augmentation and Correction代码;
2. 修改论文初稿；【OK】
截至时间：2021-10-12；

### 周工作安排（9/27-9/30）
1. 理解GAIL+SQIL，改代码，跑实验。

#### 2021/9/27
- 今日工作：
1. 在服务器215上跑N=3和5的实验（估计晚上9点能跑完）；


### 周工作安排（9/22-9/24）
1. 下载代码；
   Adversarial Imitation Learning with Trajectorial Augmentation and Correction【finish】
2. 结合增广轨迹修改上述代码；
3. 修改论文初稿（9/23）;【finish】

#### 2021/9/26
- 今日工作：
1. 改论文；
2. 跑svhn_resnet18代码+搞服务器环境；
- 明日安排：
1. 搞服务器环境，跑实验；
2. 理解GAIL代码；
3. 查svhn实验模型。

#### 2021/9/24
- 今日工作：
1. 2:30改杨文昕的材料；
2. 做核酸检测（带身份证）；
3. 改GAIL代码(20%)。
- 明日安排：
1. 改论文；
2. 改GAIL代码。
3. 写研究报告；

#### 2021/9/23
- 今日工作：
1. 看论文：PlayVirtual；
2. 下午2：30 专题辅导报告会 学院509会议室。
3. 改论文（20%）。
- 明日安排：
1. 2:30改杨文昕的材料；
2. 做核酸检测（带身份证）；
3. 跑和改SQIL代码；
4. 改论文；

#### 2021/9/22
- 今日工作：
1. 看论文（10%）：PlayVirtual；
2. 下载代码：DAugGI，安装出问题了，python=3.5，安装mj-envs失败。
- 明日安排：
1. 看论文；
2. 下午2：30 专题辅导报告会 学院509会议室。
3. 改论文；